# MLLM-DataEngine Pipeline

The Multimodal Large Language Model (MLLM)-DataEngine pipeline consists of five main steps:

- **Pre-Step** - Question Classification: We've already pre-processed this step for you. Involves classifying the validation set of A-OKVQA into 18 classes using GPT-4.
- **Step 1** - Model Inference: This involves running model inference to identify 'bad cases'.
- **Step 2** - Query Construction: Employs the classified 'bad cases' along with a prompt template to construct instructions.
- **Step 3** - QA Generation: Uses constructed queries to direct GPT-4 in generating QAs.
- **Step 4** - Post-processing: The QAs generated by GPT-4 are then post-processed into training data for fine-tuning.
- **Step 5** - Model Fine-tuning: Uses the post-processed training data to fine-tune the model.

You can find all demo data in the `engine_pipeline/data_demo` directory.

## Pre-Step: Question Classification (For A-OKVQA)

In this step, we use GPT-4 to classify the questions in the A-OKVQA validation set into 18 categories. For further details regarding these 18 categories, please refer to the provided [paper](https://arxiv.org/pdf/2308.13566.pdf) or explore the code directly.

We have already classified the A-OKVQA validation set, and you can download the output from [Google Drive]([https://drive.google.com/file/d/1RCQbCTIcdwqTJSmrZYlocXP87aDH3hgn/view?usp=drive_link](https://drive.google.com/file/d/1RE8nyVzXhIG7GMrYyKiQv10vY-gud2TK/view?usp=drive_link)). Therefore, there's no need to rerun this code if you're using A-OKVQA; you can simply use the downloaded classified validation set during model inference.

For reference, here is the script we executed to classify the A-OKVQA validation set:

```bash
python Pre_step_bad_case_classify.py \
    --input path/to/aokvqa/val/aokvqa_val.json \
    --image root/path/to/image/folder \
    --output path/to/aokvqa/val/aokvqa_v1p0_val_classified.json
```

Note: You'll need to update line 14 in `engine_pipeline/utils.py` with your own OPENAI key to access GPT-4.

## Step 1: Model Inference

Detailed information on model inference can be found in the parent [README](../README.md).

After the inference process, two output files will be generated:

1. `bad_case_aokvqa_classified.json`: Contains the 'bad cases' identified from the validation set, which are later used as few-shot examples for query construction during the data generation process.
2. `weight.txt`: Sets the data generation proportion for each question type. The weight is decided by the score your model achieves for each question type, calculated using the formula 'weight = 1 - score'.

Both output files will be utilized in the next step.

## Step 2: Query Construction

This step constructs queries for GPT-4 to generate data. The classified 'bad cases' are used as few-shot examples. Additionally, CLIP is used to select suitable images from the COCO dataset while a prompt template helps manage the instruction.

Following the example command below:

```bash
srun --quotatype=auto --gres=gpu:1 python Step2_query_construct.py \
    --input path/to/classified/bad/cases/bad_case_aokvqa_classified.json \
    --COCO_dataset path/to/COCO/dataset/folder \
    --COCO_embeding path/to/COCO/embeding/coco_images.pth \
    --output path/to/constructed/query/gptvqa_prompt.json \
    --weight path/to/weight/weight.json \
    --maxnum 6000 \
    --topk 1000
```

The input data is the classified bad cases achieved from validation set (one of the output file from model infer), which is demonstrated in engine_pipeline/data_demo/bad_case_aokvqa_classified.json, the output data will be constructed query, which is demonstrated in engine_pipeline/data_demo/gptvqa_prompt.json.

The weight is demonstrated in engine_pipeline/data_demo/weight.txt. It is calculated from the score your model achieve for each question type (the other one of the output file from model infer). You can also adjust the weight in your own way, such as the number of bad cases. If not specified, all types of questions will be sampled with equal weights.

Please download the `coco_image.pth` file from this [Google Drive](https://drive.google.com/file/d/150lBSs-cJiL1sznd5Ha9JO10sOrmZErp/view?usp=drive_link).

'maxnum' is the total number of generated QA. 'topk' is the number of top k similar images in CLIP.

## Step 3: Generate QA

With the constructed queries, we prompt GPT-4 to generate QAs. Begin by executing `engine_pipeline/Step3_generate_qa.py`, following the example command below:

```bash
python engine_pipeline/Step3_generate_qa.py \
    --input path/to/constructed/query/gptvqa_prompt.json \
    --output path/to/generated/QA/gptvqa_result.jsonl
```

The input data is the constructed query, which is demonstrated in engine_pipeline/data_demo/gptvqa_prompt.json. The output data will be the generated QA by GPT-4, which is demonstrated in engine_pipeline/data_demo/gptvqa_result.jsonl. 

Remember to update line 14 in `engine_pipeline/utils.py` with your own OPENAI key to access GPT-4.

If your code execution halts midway, simply run it again will automaticly resume the generation process from where it paused.

## Step 4: Post-processing

Following the QA generation with GPT-4, post-processing is necessary to prepare the data for fine-tuning. This step filters out QAs with "SKIP" as their answers and also those with references to 'bounding box' and 'image description' in their answers. Afterwards, we organize the data in the format of A-OKVQA.

For this, execute `engine_pipeline/Step4_post_process.py` with the example command below:

```bash
python engine_pipeline/Step4_post_process.py \
    --input path/to/generated/QA/gptvqa_result.jsonl \
    --output path/to/post_processed/training/data/gptvqa_train_data.json
```

The input data is the generated QA by GPT-4, which is demonstrated in engine_pipeline/data_demo/gptvqa_result.jsonl. The output data is post-processed training data, which is demonstrated in engine_pipeline/data_demo/gptvqa_train_data.json.

## Step 5: Model Fine-tuning

Find more details about the model fine-tuning process in the parent [README](../README.md).

After completing this step, you can revert to Step 1 to evaluate your model's performance.
