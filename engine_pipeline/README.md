# MLLM-DataEngine Pipeline

The Multimodal Large Language Model (MLLM)-DataEngine pipeline consists of five main steps:

- **Pre-Step** - Question Classification: We've already pre-processed this step for you. Involves classifying the validation set of A-OKVQA into 18 classes using GPT-4.
- **Step 1** - Model Inference: This involves running model inference to identify 'bad cases'.
- **Step 2** - Query Construction: Employs the classified 'bad cases' along with a prompt template to construct instructions.
- **Step 3** - QA Generation: Uses constructed queries to direct GPT-4 in generating QAs.
- **Step 4** - Post-processing: The QAs generated by GPT-4 are then post-processed into training data for fine-tuning.
- **Step 5** - Model Fine-tuning: Uses the post-processed training data to fine-tune the model.

You can find all demo data in the `engine_pipeline/data_demo` directory.

## Pre-Step: Question Classification (For A-OKVQA)

In this step, we use GPT-4 to classify the questions in the A-OKVQA validation set into 18 categories. For further details regarding these 18 categories, please refer to the provided [paper](https://arxiv.org/pdf/2308.13566.pdf) or explore the code directly.

We have already classified the A-OKVQA validation set, and you can download the classified A-OKVQA val from [Google Drive](https://drive.google.com/file/d/1912dPJJkVMi7is3oWw_RX59gLekdSdX4/view). Therefore, there's no need to rerun this code if you're using A-OKVQA; you can simply use the downloaded classified validation set during model evaluation.

For reference, here is the script we executed to classify the A-OKVQA validation set:

```bash
python pre_step_bad_case_classify.py \
    --input path/to/aokvqa/val/aokvqa_val.json \
    --image root/path/to/image/folder \
    --output path/to/aokvqa/val/aokvqa_v1p0_val_classified.json
```

Note: You'll need to update line 14 in `engine_pipeline/utils.py` with your own OPENAI key to access GPT-4.

## Step 1: Model Evaluation

Detailed information on model evaluation can be found in the parent [README](../README.md).

After the evaluation process, two output files will be generated:

1. `bad_case_aokvqa_classified.json`: Contains the 'bad cases' identified from the validation set, which are later used as few-shot examples for query construction during the data generation process.
2. `weight.txt`: Sets the data generation proportion for each question type. The weight is decided by the score your model achieves for each question type, calculated using the formula 'weight = 1 - score'.

Both output files will be utilized in the next step.

## Step 2: Query Construction

This step constructs queries for GPT-4 to generate data. The classified 'bad cases' are used as few-shot examples. Additionally, CLIP is used to select suitable images from the COCO dataset while a prompt template helps manage the instruction. This step requires [CLIP](https://github.com/openai/CLIP), following official instructions to install.

Following the example command below:

```bash
python step2_query_construct.py \
    --input path/to/classified/bad/cases/bad_case_aokvqa_classified.json \
    --COCO_dataset path/to/COCO/dataset/folder \
    --COCO_embeding path/to/COCO/embeding/coco_images.pth \
    --output path/to/constructed/query/gptvqa_prompt.json \
    --weight path/to/weight/weight.json \
    --maxnum 6000 \
    --topk 1000
```

The input data is the classified bad cases achieved from validation set (one of the output file from model infer), which is demonstrated in engine_pipeline/data_demo/bad_case_aokvqa_classified.json, the output data will be constructed query, which is demonstrated in `engine_pipeline/data_demo/gptvqa_prompt.json`.

The weight is demonstrated in engine_pipeline/data_demo/weight.txt. It is calculated from the score your model achieve for each question type (the other one of the output file from model infer). You can also adjust the weight in your own way, such as the number of bad cases. If not specified, all types of questions will be sampled with equal weights.

Please download the `coco_image.pth` file from this [Google Drive](https://drive.google.com/file/d/150lBSs-cJiL1sznd5Ha9JO10sOrmZErp/view?usp=drive_link).

`maxnum` is the total number of generated QA. `topk` is the number of top k similar images in CLIP.

## Step 3: Generate QA

With the constructed queries, we prompt GPT-4 to generate QAs. Begin by executing `engine_pipeline/step3_generate_qa.py`, following the example command below:

```bash
python engine_pipeline/step3_generate_qa.py \
    --input path/to/constructed/query/gptvqa_prompt.json \
    --output path/to/generated/QA/gptvqa_result.jsonl
```

The input data is the constructed query, which is demonstrated in `engine_pipeline/data_demo/gptvqa_prompt.json`. The output data will be the generated QA by GPT-4, which is demonstrated in engine_pipeline/data_demo/gptvqa_result.jsonl. 

Remember to update line 14 in `engine_pipeline/utils.py` with your own OPENAI key to access GPT-4.

If your code execution halts midway, simply run it again will automaticly resume the generation process from where it paused.

## Step 4: Post-processing

Following the QA generation with GPT-4, post-processing is necessary to prepare the data for fine-tuning. This step filters out QAs with "SKIP" as their answers and also those with references to 'bounding box' and 'image description' in their answers. Afterwards, we organize the data in the format of A-OKVQA.

For this, execute `engine_pipeline/step4_post_process.py` with the example command below:

```bash
python engine_pipeline/step4_post_process.py \
    --input path/to/generated/QA/gptvqa_result.jsonl \
    --output path/to/post_processed/training/data/gptvqa_train_data.json
```

The input data is the generated QA by GPT-4, which is demonstrated in `engine_pipeline/data_demo/gptvqa_result.jsonl`. The output data is post-processed training data, which is demonstrated in `engine_pipeline/data_demo/gptvqa_train_data.json`.

## Step 5: Model Fine-tuning

Find more details about the model fine-tuning process in the parent [README](../README.md).

Before model fine-tuning, refer to the data preparation section in [README](../README.md) and prepare data like follows:

```
.
├── A-OKVQA
│   ├── aokvqa_v1p0_train.json
│   └── aokvqa_v1p0_val_classified.json  # with question type assigned by GPT-4
├── cc_sbu_align
│   ├── filter_cap.json
│   └── image
├── COCO2017
│   ├── annotations
│   │    └── ...
│   ├── train2017
│   │    └── ...
│   └── val2017
│        └── ...
└── gptvqa
    ├── DataEngine_round1_data.json
    └── DataEngine_round2_data.json
    
```

To use generated data into model fine-tuning, there are several things you should do:

**1. add dataset config**

Dataset configs are put under ```minigpt4/configs/datasets/```. Each dataset config consists of a folder and some yaml configs under this folder, such as ```gptvqa_round1/defaults.yaml```. To add another dataset, create a new folder and yaml config under this folder. Use GPTVQA config ```gptvqa_round1/defaults.yaml``` we provided as template:

```yaml
datasets:
  the_name_of_your_data:
    data_type: images
    build_info:
      train:
          annotation: ./data/gptvqa/DataEngine_round1_data.json
          coco: ./data/COCO2017/annotations/instances_train2017.json
          images: ./data/COCO2017/train2017
```

Set ```annotation``` to your generated QA json and change the dataset name.

**2. add dataset builder**

After step1, add dataset builder in ```minigpt4/datasets/builders/image_text_pair_builder.py```. Just follow the code of GPTVQA builder (```GPTVQARound1Builder```) and add a new builder pointed to your generated data. Set ```DATASET_CONFIG_DICT``` to the yaml config of your generated data and remember changing the register name of dataset in decorator.

**3. add dataset into train config**

Finally, to add your generated data into model fine-tuning, just add data into train config like this:

```yaml
the_name_of_your_data:
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    format: "QM-A"
    sample_ratio: 70
```

Be sure to change the dataset name and set proper sample ratio (the square root of the amount of data follow Instruct-BLIP).

After model fine-tuning using your generated data, you can revert to Step 1 to evaluate your model's performance.
